{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "sys.path.append(os.path.dirname(find_dotenv()))\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "#Get the path for the data\n",
    "PATH = os.getenv('DATA_PATH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_scripts.file_handler import write_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_label_string(token,label_lower):\n",
    "    token = token.replace('<' + label_lower + '>', '')\n",
    "    token = token.replace('</' + label_lower + '>', '')\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['First_Name', 'Last_Name', 'Phone_Number', 'Age', 'Full_Date', 'Date_Part', 'Health_Care_Unit', 'Location']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(X,Y,keep_labels=True):\n",
    "    X_unique = []\n",
    "    Y_unique = []\n",
    "\n",
    "    for x,y in zip(X,Y):\n",
    "        if x not in X_unique:\n",
    "            X_unique.append(x)\n",
    "            Y_unique.append(y)\n",
    "        #check if the list y contain named entity start with B-{entity}\n",
    "        elif x in X_unique and any([\"B-\"+label in y for label in labels]):\n",
    "            if(keep_labels):\n",
    "                X_unique.append(x)\n",
    "                Y_unique.append(y)\n",
    "\n",
    "    return X_unique,Y_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def preprocessing(IOB=False,punctuation=string.punctuation,no_duplicates=False):\n",
    "    #Load file and get lines\n",
    "    with open(PATH) as f:\n",
    "        documents = f.read().splitlines() \n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    #for couting the nr of inside labels\n",
    "    nr_of_inside_labels = {\n",
    "        'First_Name': 0,\n",
    "        'Last_Name': 0,\n",
    "        'Phone_Number': 0,\n",
    "        'Age': 0,\n",
    "        'Full_Date': 0,\n",
    "        'Date_Part': 0,\n",
    "        'Health_Care_Unit': 0,\n",
    "        'Location': 0\n",
    "    }\n",
    "\n",
    "    for doc in documents:\n",
    "        curr_X = []\n",
    "        curr_Y = []\n",
    "        \n",
    "        #to lowercase\n",
    "        doc = doc.lower()\n",
    "        \n",
    "        #add spaces between named entities\n",
    "        doc = doc.replace(\">\",\"> \")\n",
    "        doc = doc.replace(\"<\",\" <\")\n",
    "\n",
    "        #add spaces \n",
    "        doc = doc.replace(\"=\",\" = \")\n",
    "        doc = doc.replace(\"*\",\" * \")\n",
    "        doc = doc.replace(\"+\",\" + \")\n",
    "        \n",
    "        doc = doc.replace(\"(\",\" (\")\n",
    "        doc = doc.replace(\")\",\") \")\n",
    "        \n",
    "        doc = doc.replace(\"->\",\" ->\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Add space after dot and comma when followed by a letter\n",
    "        doc = re.sub(r'(?<=[.,:])(?=[a-zA-Z])', r' ', doc)\n",
    " \n",
    "        #split string\n",
    "        words = doc.split()\n",
    "        \n",
    "        #Skip empty lines\n",
    "        if len(words) <= 1:\n",
    "            continue\n",
    "        \n",
    "        named_entity = False\n",
    "        inside_entity = False\n",
    "        \n",
    "        #loop over words, and mark each word as O or as their specific label\n",
    "        for word in words:    \n",
    "\n",
    "            #check if current token is a named entity\n",
    "            if '<' in word[0] and '>' in word[-1]:\n",
    "                \n",
    "                #find the correct label\n",
    "                for label in labels:\n",
    "                    label_lower = label.lower()\n",
    "\n",
    "                    #start of entity\n",
    "                    if '<' + label_lower + '>' in word:\n",
    "                        word = clean_label_string(word,label_lower)\n",
    "                        named_entity = True\n",
    "                        break\n",
    "                    #end of entity\n",
    "                    elif '</' + label_lower + '>' in word:\n",
    "                        word = clean_label_string(word,label_lower)\n",
    "                        named_entity = False\n",
    "                        inside_entity = False\n",
    "                        break\n",
    "                        \n",
    "            #skip if empty string\n",
    "            word = word.strip()\n",
    "            word = word.strip(punctuation)\n",
    "            \n",
    "            #skip if empty string\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            \n",
    "            #start of named entity    \n",
    "            if named_entity and not inside_entity:\n",
    "                if IOB:\n",
    "                    curr_Y.append('B-'+label)\n",
    "                else:\n",
    "                    curr_Y.append(label)\n",
    "                curr_X.append(word)\n",
    "                inside_entity = True\n",
    "\n",
    "            #inside of named entity\n",
    "            elif named_entity and inside_entity:\n",
    "                nr_of_inside_labels[label] += 1\n",
    "\n",
    "                if IOB:\n",
    "                    curr_Y.append('I-'+label)\n",
    "                else:\n",
    "                    curr_Y.append(label)\n",
    "                curr_X.append(word)\n",
    "        \n",
    "            #outside of named enitity\n",
    "            else:     \n",
    "                curr_Y.append('O')\n",
    "                curr_X.append(word)\n",
    "\n",
    "        X.append(curr_X)\n",
    "        Y.append(curr_Y)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    if no_duplicates:\n",
    "        X,Y = remove_duplicates(X,Y,keep_labels=True)\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#puncation without - and >\n",
    "punctuation = string.punctuation.replace('-','')\n",
    "punctuation = punctuation.replace('>','')\n",
    "\n",
    "X, Y = preprocessing(IOB=True,punctuation=punctuation)\n",
    "X_no_duplicates, Y_no_duplicates = preprocessing(IOB=True,punctuation=punctuation,no_duplicates=True)\n",
    "\n",
    "print('Number of sentences: ',len(X))\n",
    "print('Number of sentences without duplicates: ',len(X_no_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_file(filename=\"clean_iob\",X=X,Y=Y)\n",
    "write_csv_file(filename=\"clean_iob_no_duplicates\",X=X_no_duplicates,Y=Y_no_duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "947566739ad5f995ef683c347463316d267e00143e5dd2f059640c7a26e2b5e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
