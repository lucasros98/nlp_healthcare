{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = \"../data/data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the named entities for each label in the data into an array.\n",
    "\n",
    "Prints them in a dataframe, together with the number of found entities for each label.\n",
    "\n",
    "Also prints a dataframe with the unique entities for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(named_entities):\n",
    "    for entity in named_entities:\n",
    "        named_entities[entity][0] = list(set(named_entities[entity][0]))\n",
    "        named_entities[entity][1] = len(named_entities[entity][0])\n",
    "    \n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_to_dataframe(named_entities, cols):\n",
    "    return pd.DataFrame.from_dict(named_entities, orient='index', columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    labels = ['First_Name', 'Last_Name', 'Phone_Number', 'Age', 'Full_Date', 'Date_Part', 'Health_Care_Unit', 'Location']    \n",
    "        \n",
    "    found_entities = {}\n",
    "    start_index = text.find(\"<\")\n",
    "    while start_index != -1:\n",
    "        end_index = text.find(\">\", start_index)\n",
    "        end_index2 = text.find(\"</\", end_index)\n",
    "        \n",
    "        named_entity = text[start_index+1:end_index]\n",
    "        named_entity_value = text[end_index+1:end_index2]\n",
    "        \n",
    "        end_index2 = text.find(\">\", end_index2)\n",
    "        start_index = text.find(\"<\", end_index2)\n",
    "\n",
    "        if not named_entity in labels: continue\n",
    "\n",
    "        if named_entity in found_entities:\n",
    "            found_entities[named_entity][0].append(named_entity_value)\n",
    "            found_entities[named_entity][1] += 1\n",
    "        else:\n",
    "            found_entities[named_entity] = [[named_entity_value], 1]\n",
    "        \n",
    "    return found_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(title, table):\n",
    "    print(\"\\n\\n\", title.upper(), \"\\n\\n\", table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(label, df):\n",
    "    values = df_unique.loc[label, \"Value\"]\n",
    "    sorted_values = sorted(values, key=len)\n",
    "    for val in sorted_values:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique(df_all, df_unique):\n",
    "    print_data = {\n",
    "        'Label': df_all.axes[0],\n",
    "        'Entities': df_all.loc[:,\"Count\"],\n",
    "        'Unique Entities': df_unique.loc[:,\"Count\"],\n",
    "        '% Unique': round((df_unique.loc[:,\"Count\"] / df_all.loc[:,\"Count\"])*100, 1)\n",
    "    }\n",
    "\n",
    "    print_table(\"unique entities\", tabulate(print_data, headers=\"keys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common(df):\n",
    "    results = []\n",
    "    for label in df.axes[0]:\n",
    "        entities = df.loc[label, \"Value\"]\n",
    "        counter = Counter(entities)\n",
    "        most_common_item = counter.most_common(1)[0]\n",
    "        results.append([\n",
    "            label,\n",
    "            most_common_item[0],\n",
    "            most_common_item[1],\n",
    "            round(most_common_item[1] / len(entities) * 100, 2)\n",
    "        ])\n",
    "        \n",
    "    print_table(\"most common entity\", tabulate(results, headers=[\"Label\", \"Entity\", \"Count\", \"Percentage\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common_six(df):\n",
    "    results = []\n",
    "    for label in df.axes[0]:\n",
    "        entities = df.loc[label, \"Value\"]\n",
    "        results.append([\n",
    "            label,\n",
    "            [item for item, count in Counter(entities).most_common(6)]\n",
    "        ])\n",
    "        \n",
    "    print_table(\"six most common entities\", tabulate(results, headers=[\"Label\", \"Entities\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_count(label, n):\n",
    "    results = []\n",
    "    entities = df_all.loc[label, \"Value\"]\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(entities)\n",
    "\n",
    "    # Sort the words by frequency\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract the top n words\n",
    "    top_n = sorted_words[:n]\n",
    "    \n",
    "    print_table(f\"{n} most common: {label}\", tabulate(top_n, headers=[\"Entity\", \"Count\"]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wordclouds(label1, label2):\n",
    "    words = df_all.loc[label1, \"Value\"]\n",
    "    words2 = df_all.loc[label2, \"Value\"]\n",
    "\n",
    "    word_frequency = dict(Counter(words))\n",
    "    word_frequency2 = dict(Counter(words2))\n",
    "\n",
    "    wordcloud1 = WordCloud(width = 800, height = 800, background_color='white',\n",
    "                min_font_size = 10).generate_from_frequencies(word_frequency)\n",
    "    \n",
    "    wordcloud2 = WordCloud(width = 800, height = 800, background_color='white',\n",
    "                min_font_size = 10).generate_from_frequencies(word_frequency2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    ax1.imshow(wordcloud1) \n",
    "    ax1.axis(\"off\") \n",
    "    ax2.imshow(wordcloud2) \n",
    "    ax2.axis(\"off\") \n",
    "\n",
    "    print(\"\\n\\nWORD CLOUDS\")\n",
    "    \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_most_common_from_csv(n, file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        headers = ['Name', 'Count']\n",
    "        data = [row for row in reader if int(row[1]) >= 10000]\n",
    "\n",
    "    data = sorted(data, key=lambda x: int(x[1]), reverse=True)\n",
    "    print_table(f\"{n} most common in Sweden\", tabulate(data[:n], headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = extract_named_entities(file_path)\n",
    "df_all = named_entities_to_dataframe(named_entities, cols=['Value', 'Count'])\n",
    "\n",
    "\n",
    "named_entities_unique_values = remove_duplicates(named_entities)\n",
    "df_unique = named_entities_to_dataframe(named_entities_unique_values, cols=['Value', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(\"dataframe with all entities\", df_all)\n",
    "print_table(\"dataframe with unique entities\", df_unique)\n",
    "\n",
    "print_unique(df_all, df_unique)\n",
    "print_most_common(df_all)\n",
    "print_most_common_six(df_all)\n",
    "\n",
    "print_n_count(\"First_Name\", 6)\n",
    "print_n_count(\"Location\", 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_wordclouds(\"First_Name\", \"Last_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_values(\"Date_Part\", df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_values(\"Health_Care_Unit\", df_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Källa på namn SCB: https://www.scb.se/hitta-statistik/sverige-i-siffror/namnsok/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_most_common_from_csv(10, \"../data/last_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_most_common_from_csv(10, \"../data/first_names_women.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_most_common_from_csv(10, \"../data/first_names_men.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations...\n",
    "\n",
    "When goin through the data set, we have noticed quite a few instances of abbriviations being used for words in normal language, such as:\n",
    "\n",
    "- \"hö\" instead of \"höger\"\n",
    "- \"vä\" instead of \"vänster\"\n",
    "- \"fr\" instead of \"från\"\n",
    "- \"pat\" instead of \"patient\"\n",
    "- \"beh\" instead of \"behandlas\"\n",
    "- \"bed\" instead of \"bedömning\" (?)\n",
    "- \"avd\" instead of \"avdelning\"\n",
    "- \"bakt\" instead of \"bakterier / bakterie\"\n",
    "- \"rel\" instead of \"relativt\"\n",
    "- \"perm\" instead of \"permission\"\n",
    "- \"mkt\" instead of \"mycket\"\n",
    "- \"stud\" instead of \"student / studerande\"\n",
    "- \"vb\" instead of \"vid behov\" (?)\n",
    "- \"ang\" instead of \"angående\"\n",
    "- \"enl\" instead of \"enligt\"\n",
    "- \"enh\" instead of \"enhet\" (?)\n",
    "\n",
    "Also, there are instances of many medical-specific abbreviations:\n",
    "\n",
    "- \"UL\" = \"Ultraljud\"\n",
    "- \"ADL\" = \"Aktiviteter i dagliga livet\"\n",
    "- \"VAS\" = \"Visuell analog skala\" (used for indicating percived pain from 0-10)\n",
    "- \"KOL\" = \"kroniskt obstruktiv lungsjukdom\"\n",
    "- \"CIDP\" = \"Kronisk inflammatorisk demyeliniserande polyneuropati\"\n",
    "- \"AF\" = \"andningsfrekvens\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(file_name, length):\n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    two_letter_words = []\n",
    "    for line in lines:\n",
    "        words = line.strip().split(' ')\n",
    "        for word in words:\n",
    "            if len(word) == length and word.isalpha():\n",
    "                two_letter_words.append(word.lower())\n",
    "    \n",
    "    return two_letter_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_remove(word_list, file_name):\n",
    "    found_words = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.split(';')[0].lower()\n",
    "            if word in word_list:\n",
    "                word_list.remove(word)\n",
    "                found_words.append(word)\n",
    "    return found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = extract_words(file_path, 3)\n",
    "print(len(words))\n",
    "unique_words = list(set(words))\n",
    "print(\"unique:\", len(unique_words))\n",
    "found_words = check_and_remove(unique_words, '../data/terms.csv')\n",
    "print(\"found in terms:\", len(found_words))\n",
    "print(\"unique after: \", len(unique_words))\n",
    "print(unique_words)\n",
    "\n",
    "#använd vokabulär över vanliga ord..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentence_lengths(file_name):\n",
    "    sentence_lengths = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            sentence_lengths.append(len(line.strip()))\n",
    "    \n",
    "    plt.hist(sentence_lengths, bins=50, range=[0, 900])\n",
    "    plt.xlabel(\"Sentence Length\")\n",
    "    plt.ylabel(\"Number of Sentences\")\n",
    "    plt.title(\"Histogram of Sentence Lengths\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentence_lengths(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_word(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        first_words = [line.split()[0] if line.strip() else '' for line in file]\n",
    "    return sorted(first_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = extract_first_word(file_path)\n",
    "unique_categories = sorted(list(set(categories)))\n",
    "print(\"total categories:\",len(categories))\n",
    "print(\"unique categories:\", len(unique_categories))\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
